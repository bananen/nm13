\documentclass[a4paper]{scrreprt}

\usepackage[german]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{ae}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{hyperref}

\begin{document}

\title{Numerik Zusammenfassung}
\author{Fedor Scholz}
\maketitle

\tableofcontents
\vspace{1cm}

\chapter{Lineare Gleichungssysteme}

Problem: Finde Loesungsvektor x fuer\\\\
$Ax = b$\\\\
Das Problem besitzt genau dann eine eindeutige Loesung x*, wenn A invertierbar ist.
Theoretische Loesung: $x* = A^{-1}b$

\section{Gaußsches Eliminationsverfahren}

Problem: Finde Loesungsvektor x fuer\\\\
$Ax = b$, wobei A invertierbar\\\\
Fuer jede invertierbare Matrix A existiert eine Permutationsmatrix P derart, dass eine (dazu) eindeutige Dreieckszerlegung $PA = LR$ moeglich ist, wobei R eine obere Dreickecksmatrix und L eine untere unipotente Dreiecksmatrix ist.

\subsection{Algorithmus}

\begin{itemize}
	\item Bestimme Matrizen P, L und R mit $PA = LR$
	\item Loese $Lc = Pc$ (Vorwaertssubstitution)
	\item Loese $Rx = c$ (Rueckwaertssubstitution)
\end{itemize}

Aufwand der LR-Zerlegung: $\frac{n^3}{3} - \frac{n}{3}$

\section{Einschub: Gleitpunktrechnung, Matrixnormen}

\subsection{Gleitpunktrechnung}

Darstellung einer Gleitpunktzahl: $z = a * d^e$ mit $e_{min} \leq e \leq e_{max}$

\subsection{Matrixnormen}

Eine Abbildung von einem Vektorraum in die reellen Zahlen heißt eine Norm, wenn gilt:
\begin{itemize}
	\item $||v|| \geq 0$ und ($||v|| = 0 \Leftrightarrow v = 0$) (positive Definitheit)
	\item $||\alpha v|| = |\alpha|||v||$ (Homogenitaet)
	\item $||v_1 + v_2|| \leq ||v_1|| + ||v_2||$ (Dreiecksungleichung)
\end{itemize}

Eine Matrixnorm passt zu einer gegebenen Vektornorm genau dann, wenn\\
$||Ax|| \leq ||A||||x||$\\\\

Sei $||.||$ eine beliebige Norm, dann definieren wir die zugehoerige Matrixnorm als\\
$||A|| := \sup\limits_{x \neq 0}\frac{||Ax||}{||x||}$\\\\

\section{Kondition linearer Gleichungssysteme}

$cond(A) := ||A||||A^{-1}||$\\\\

Eigenschaften:\\
$cond(A) = cond(\alpha A)$\\
$cond(A) = \frac{max_{||y|| = 1}||Ay||}{min_{||z|| = 1}||Az||}$

\section{Cholesky-Verfahren fuer symmetrische, positiv definite Matrizen}

Sei A symmetrisch und positiv definit, dann gilt fuer die Zerlegung $A = LR$: $R = DL^T$, wobei D eine positiv definite Diagonalmatrix ist. Da $D = diag(d_i)$ positiv definit, existiert $D^\frac{1}{2} = diag(\sqrt{d_i})$ und daher die Cholesky-Zerlegung:\\\\
$A = \overline{L}\overline{L}^T$\\\\
mit unterer Dreiecksmatrix $\overline{L} = LD^\frac{1}{2}$

\subsection{Algorithmus}
\begin{itemize}
	\item Bestimme mit dem Cholesky-Verfahren $\overline{L}$ mit $A = \overline{L}\overline{L}^T$ (Cholesky-Zerlegung)
	\item Loese $\overline{L}c = b$ (Vorwaertssubstitution)
	\item Loese $\overline{L}^Tx = c$ (Rueckwaertssubstitution)
\end{itemize}

Aufwand: $\frac{1}{6}n^3$

\section{QR-Zerlegung}

Zu Matrix $A \in \mathbb{R}^{mxn}$ mit $m \geq n$ kontruieren wir Zerlegung\\\\
$A = QR$\\\\
mit orthogonaler Matrix Q und\\
$R = \begin{pmatrix}\overline{R}\\0\end{pmatrix}$, $\overline{R}$ obere Dreiecksmatrix

\subsection{Algorithmus}
\begin{itemize}
	\item Bestimme Matrizen Q und R mittels Householder-Transformationen mit $A = QR$ (QR-Zerlegung)
	\item Loese $Qc = b$ ($Q^{-1} = Q^T$, also $c = Q^Tb$)
	\item Loese $Rx = c$ (Rueckwaertssubstitution)
\end{itemize}

\section{Lineare Ausgleichsprobleme}

Betrachte ueberbestimmtes Gleichungssystem $Ax = b$, mit $A \in \mathbb{R}^{mxn}$, $m > n$, dieses hat im Allgemeinen keine Loesung. Der Vektor x ist genau dann eine Loesung des linearen Ausgleichsproblems $||Ax - b||_2 = min$, falls er die so genannte Normalengleichung\\\\
$A^TAx = A^Tb$\\\\
erfuellt. Es ist genau dann eindeutig loesbar, wenn der Rang A maximal ist ($Rang(A) = n$).\\\\
Ist der Rang von A maximal, so ist $A^TA$ eine symmetrische positiv definite Matrix.\\\\

Seien $A \in \mathbb{R}^{mxn}$ mit $m \geq n$ eine Matrix mit vollem Rang und Q und R die Matrizen der QR-Zerlegung von A, d.h. $Q^TA = R = \begin{pmatrix}\overline{R}\\0\end{pmatrix}$, mit invertierbarer Matrix $\overline{R}$. Dann ist $x = \overline{R}^{-1}c$ die Loesung des linearen Ausgleichsproblem $||Ax - b||_2 = min$, wobei c definiert ist durch $Q^Tb = \begin{pmatrix}c\\d\end{pmatrix}$.

\subsection{Algorithmus}
\begin{itemize}
	\item Bestimme Matrizen Q und R mittels Householder-Transformationen mit $A = QR$ (QR-Zerlegung)
	\item Berechne $Q^Tb = \begin{pmatrix}c\\d\end{pmatrix}$
	\item Loese $\overline{R}x = c$ (Rueckwaertssubstitution)
\end{itemize}

\chapter{Nichtlineare Gleichungssysteme}

\section{Fixpunktiteration}

Problem: Fuer vorgegebene Abbildung $F: D \subset \mathbb{R}^n \rightarrow \mathbb{R}^n$ finde x mit $F(x) = x$. Dieses x heißt Fixpunkt. Das Problem ist aequivalent zu $-Af(x) = 0$, falls A invertierbar. Somit ist $f(x) = 0$ aequivalent zu $F(x) = x$ mit $F(x) := x - Af(x)$.\\\\

Idee der Fixpunktiteration: $x_{k+1} = F(x_k)$, wobei diese Folge gegen den Fixpunkt $x^*$ konvergiert.\\\\

Eine Abblidung $F: D \rightarrow D \subset \mathbb{R}^n$ ist eine Kontraktion auf D, falls ein $0 \leq \Theta < 1$ existiert mit $||F(x) - F(y)|| \leq \Theta ||x-y||$

\subsection{Banachscher Fixpunktsatz}
Es sei $F: D \rightarrow D$ eine Kontraktion auf D, D abgeschlossene Teilmenge des $\mathbb{R}^n$, mit Kontraktionszahl $0 \leq \Theta < 1$. Dann gilt:
\begin{itemize}
	\item Es existiert genau ein Fixpunkt $x^*$ von F
	\item Die durch die Vorschrift $x_{k+1} = F(x_k)$ definierte Folge konvergiert gegen $x^*$ fuer jeden Startwert $x_0 \in D$.
	\item Es gelten die Abschaetzungen
		\begin{itemize}
			\item $||x^* - x_k|| \leq \Theta ||x^* - x_{k-1}||$ (lineare Konvergenz)
			\item $||x^* - x_k|| \leq \frac{\Theta}{1 - \Theta} ||x_0 - x_1||$ (a priori-Abschaetzung)
			\item $||x^* - x_k|| \leq \frac{\Theta}{1 - \Theta} ||x_{k-1} - x_k||$ (a posteriori-Abschaetzung)
		\end{itemize}
\end{itemize}

\section{Das Newton-Verfahren}

\subsection{Praktische Durchfuehrung}
Waehle Startwert $x_0$\\
while $(|| \Delta x_k)|| >$ TOL do\\
\hspace*{10mm} Loese $f'(x_k) \Delta x_k = -f(x_k)$ (lin. Gleichungssystem, berechne LR-Zerlegung)\\
\hspace*{10mm} Berechne $x_{k+1} = x_k + \Delta x_k$\\

\subsection{Praktische Durchfuehrung des vereinfachten Newton-Verfahrens}
Waehle Startwert $x_0$ und berechne LR-Zerlegung von $A \approx f'(x_0)$\\
while $(|| \Delta x_k)|| >$ TOL do\\
\hspace*{10mm} Loese $A \Delta x_k = -f(x_k)$\\
\hspace*{10mm} Berechne $x_{k+1} = x_k + \Delta x_k$

\chapter{Interpolation und Approximation}

\section{Polynominterpolation}

Gegeben sind $n + 1$ Stuetzwerte $f_i:=f(x_i)$ einer Funktion f an den Stuetzstellen $x_0 < x_1 < ... < x_n$. Suche Polynom $p(x)$ vom Grad $\leq n$ mit $p(x_i) = f_i$ fuer $i = 0,...,n$. Ein Polynom vom Grad $\leq n$ ist durch Vorgabe von $n + 1$ Punkten eindeutig bestimmt.\\

Lagrangsche Interpolationsformel: Zu $n+1$ Stuetzpunkten existiert genau ein Interpolationspolynom $p(x)$ vom Grad $\leq n$, welches gegeben ist durch $p(x) = \sum_{i=0}^n f_i L_i(x)$, wobei $L_i(x)$ die Lagrange-Polynome der Stuetzstellen $x_i$ sind: $L_i(x) = \prod_{j=0, j \neq i}^n \frac{x - x_j}{x_i - x_j}$.

\subsection{Newtonsche Interpolationsformel / Dividierte Differenzen}

Wir schreiben:\\
$p(f|x_0,...,x_n) = a_0 + a_1(x - x_0) + a_2(x - x_0)(x - x_1) + ... + a_n(x - x_0)*...*(x - x_{n-1})$.

Darstellungen eines Polynoms vom Grad $\leq n$:
\begin{itemize}
	\item $p(x) = c_0 + c_1x + ... + c_{n-1}x^{n-1} + c_nx^n$ zur Basis der Monome
	\item $p(x) = b_0L_0(x) + b_1L_1(x) + ... + b_nL_n(x)$ zur Lagrange-Basis
	\item $p(x) = a_0 + a_1w_1(x) + ... + a_nw_n(x)$ zur Newton-Basis mit $w_i(x) = \prod_{j=0}^{i-1}(x - x_j)$
\end{itemize}

Es gilt $a_n = c_n$.\\

Berechnung der Koeffizienten $a_i$ fuer Menschen:\\
$f_0 = p(x_0) = a_0$\\
$f_1 = p(x_1) = a_0 + (x_1 - x_0)a_1$\\
$f_2 = p(x_2) = a_0 + (x_2 - x_0)a_1 + (x_2 - x_0)(x_2 - x_1)a_2$\\

Aufwand der Koeffizientenbestimmung: n Divisionen, $n(n-1)$ Multiplikationen, $n(n+1)$ Additionen.\\

Wir nennen den Koeffizienten $a_n$ die n-te dividierte Differenz von f zu den Stuetzstellen $x_0,...,x_n$ und wir schreiben $f[x_0,...,x_n] := a_n$.\\\\

Newtonsche Interpolationsformel:\\
Zu $n+1$ Stuetzstellen $(x_i,f_i), i=0,...,n$ existiert genau ein Interpolationspolynom $p(x)$ vom Grad $\leq n$, welches gegeben ist durch\\\\
$p(x) = f[x_0] + (x-x_0)f[x_0,x_1] + ...+ (x-x_0)*...*(x-x_{n-1})f[x_0,...,x_n]$\\\\wobei die dividierten Differenzen gegeben sind durch\\\\
$f[x_i]:=f_i, f[x_i,...,x_{i+k}] = \frac{f[x_i,...,x_{i+k-1}]-f[x_{i+1},...,x_{i+k}]}{x_i - x_{i+k}}$\\\\

\subsection{Tschebyscheff-Interpolation}

Definition der Tschebyscheff-Polynome:\\
$T_0(x) = 1$\\
$T_1(x) = x$\\
$T_{n+1}(x) = 2x * T_n(x)-T_{n-1}(x)$\\

$T_n$ ist ebenfalls gegeben durch $T_n(x) = cos(n*arccos(x))$\\\\

Zu $n+1$ Stuetzstellen, wobei die Stuetzstellen genau den Nullstellen des Tschebyscheff-Polynoms $T_{n+1}$ entsprechen, laesst sich das eindeutige Interpolationspolynom $p(x) = p(f|x_0,...,x_n)$ vom Grad $\leq n$ darstellen durch\\\\
$p(x) = \frac{1}{2}c_0+c_1T_1(x) + ...+ c_nT_n(x)$\\\\
mit\\\\
$c_k = \frac{2}{n+1}\sum_{i=0}^nf_icos(k\frac{2i+1}{2n+2}\pi)$\\\\
Aufwand der Berechnung von $p(x)$ ist $n+1$ Multiplikationen und $2(n+1)$ Addition (mit dem Clenshaw-Algorithmus).

\section{Kubische Spline-Interpolation}

Problem: Suche glatte Funktion $s:[a,b]\rightarrow\mathbb{R}$, wie durch vorgegebene Punkte geht.
\begin{itemize}
	\item Interpolation: $s(x_i) = y_i$
	\item glatt: s zweimal stetig differenzierbar und $\int_a^b|s''(x)|^2dx$ minimal
\end{itemize}

Arten von Splines mit ihren zusaetchlichen Bedingungen:

\begin{itemize}
	\item Eingespannter: $s'(a) = f'(a)$ und $s'(b) = f'(b)$
	\item Natuerlicher: $s''(a) = 0$ und $s''(b) = 0$
	\item Periodischer: $s^{(k)}(a) = s^{(b)}(b)$ fuer $k = 0,1,2$ und $f'(a) = f'(b)$
\end{itemize}

\subsection{Konstruktion}

$d_i := y[x_i,x_{i+1}]-y[x_{i-1},x_i]$\\\\
Bei eingespannten Splines gilt außerdem:\\
$d_0 := y[x_0,x_1] - v_0$\\
$d_n := v_n - y[x_{n-1},x_n]$\\\\

Insgesamt erhalten wir das lineare Gleichungssystem\\
$\frac{1}{6}\begin{pmatrix}
2h_1 & h_1\\
h_1 & 2(h_1 + h_2) & h_2\\
    & ... & ... & ...\\
& & h_{n-1} & 2(h_{n-1}+h_n) & h_n\\
& & & h_n & 2h_n
\end{pmatrix}
\begin{pmatrix}\gamma_0\\\gamma_1\\...\\\gamma_{n-1}\\\gamma_n\end{pmatrix}=
\begin{pmatrix}d_0\\d_1\\...\\d_{n-1}\\d_n\end{pmatrix}$



\end{document}
